{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# News Scraper - InfoMoney (Batch Processing)\n\nThis notebook uses the `src.scraper` module to collect news from InfoMoney via API.\nIt iterates over a list of search terms, saves individual CSV files, and then consolidates them into a single dataset."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Fix para erro de Unicode no Windows\n",
    "os.environ[\"PYTHONUTF8\"] = \"1\"\n",
    "\n",
    "# Auto-reload para refletir mudanças nos scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add root directory to path to import src modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.scraper import get_news_from_period\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "# Configure logger to see output in the notebook\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Parameter Definition\nDefine the list of search terms, the start date for collection, and the output folder."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SEARCH_TERMS = [\"Itaú\", \"Dólar\", \"Petrobras\", \"Vale\", \"Bolsa de Valores\", \"Bolsa\", \"B3\", \"IBOVESPA\", \"Bradesco\", \"Economia\"]\nSTART_DATE = datetime(2025, 1, 1)\nEND_DATE = datetime(2025, 12, 31)\nOUTPUT_DIR = os.path.join(\"..\", \"src\", \"dataset\", \"scraper\", \"search\")\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Configured to search {len(SEARCH_TERMS)} terms between {START_DATE.date()} and {END_DATE.date()}\")\nprint(f\"Results will be saved to: {os.path.abspath(OUTPUT_DIR)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batch Collection Execution\nIterates over terms, collects data, and saves to CSV in the results folder."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for term in SEARCH_TERMS:\n    print(f\"\\n{'='*50}\")\n    print(f\"Processing term: '{term}'\")\n    print(f\"{'='*50}\")\n    \n    try:\n        # Execute scraper for the current term\n        df = get_news_from_period(term=term, start_date=START_DATE, end_date=END_DATE)\n        \n        if not df.empty:\n            # Generate filename\n            safe_term = term.replace(\" \", \"_\").lower()\n            timestamp = datetime.now().strftime('%Y%m%d')\n            filename = f\"news_{safe_term}_{timestamp}.csv\"\n            filepath = os.path.join(OUTPUT_DIR, filename)\n            \n            # Save to CSV\n            df.to_csv(filepath, index=False)\n            print(f\"\\n[SUCCESS] Saved {len(df)} news for '{term}' to: {filepath}\")\n            \n            # Optional: Display first few rows\n            display(df.head(3))\n        else:\n            print(f\"\\n[WARNING] No news found for '{term}'\")\n            \n    except Exception as e:\n        print(f\"\\n[ERROR] Failed to collect term '{term}': {e}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Consolidate Results\nReads all CSV files from the results folder, combines them into a single dataset, and removes duplicates (based on link)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- 1. Collection Execution (if needed) ---\n# Uncomment lines below to collect new data from the API\n# for term in SEARCH_TERMS:\n#    print(f\"Processing term: {term}\")\n#    get_news_from_period(term, START_DATE, END_DATE)\n\n# --- 2. Consolidation and Filtering ---\nprint(\"\\n\" + \"=\"*50 + \"\\nStarting Consolidation and Filtering...\\n\" + \"=\"*50)\n\n# List of keywords for filtering\nKEYWORDS = [\n    \"Ibovespa\", \"BOVA11\", \"Bolsa\", \"Ações\", \"Mercado\", \"Câmbio\", \"Dólar\", \"Juros\", \"Selic\", \"Inflação\",\n    \"IPCA\", \"Banco Central\", \"Copom\", \"Fazenda\", \"CDI\", \"Petrobras\", \"Vale\", \"Itaú\", \"Bradesco\", \"Banco do Brasil\",\n    \"B3\", \"Ambev\", \"Eletrobras\", \"WEG\", \"Suzano\", \"Gerdau\", \"Localiza\", \"Rumo\", \"Equatorial\", \"BTG Pactual\"\n]\n\nall_files = glob.glob(os.path.join(OUTPUT_DIR, \"news_*.csv\"))\n\nif all_files:\n    print(f\"Found {len(all_files)} files to consolidate.\")\n    \n    # Read and concatenate all files\n    df_list = [pd.read_csv(f) for f in all_files]\n    consolidated_df = pd.concat(df_list, ignore_index=True)\n    \n    total_rows = len(consolidated_df)\n    print(f\"Total rows before deduplication and filtering: {total_rows}\")\n    \n    # Drop duplicates based on 'link'\n    consolidated_df = consolidated_df.drop_duplicates(subset=['link'])\n    unique_rows = len(consolidated_df)\n    print(f\"Total unique after deduplication: {unique_rows}\")\n    \n    # --- FILTERING ---\n    print(f\"\\nApplying filter with {len(KEYWORDS)} keywords...\")\n    # Added \\b to ensure exact word match (avoids partial matches like 'privatiz-ações')\n    pattern = '|'.join([fr'\\b{k}\\b' for k in KEYWORDS])\n    # Convert to string and handle NaN before filtering\n    mask = consolidated_df['title'].astype(str).str.contains(pattern, case=False, na=False)\n    filtered_df = consolidated_df[mask].copy()\n    \n    print(f\"Total news after filtering: {len(filtered_df)}\")\n    print(f\"News removed: {unique_rows - len(filtered_df)}\")\n\n    # Sort by date\n    if 'date' in filtered_df.columns:\n        filtered_df = filtered_df.sort_values(by='date', ascending=False)\n    \n    # Save consolidated file\n    timestamp = datetime.now().strftime('%Y%m%d')\n    # Save to parent directory of OUTPUT_DIR (dataset/infomoney)\n    parent_dir = os.path.dirname(OUTPUT_DIR)\n    output_file = os.path.join(parent_dir, f\"consolidated_news_{timestamp}.csv\")\n    filtered_df.to_csv(output_file, index=False)\n    \n    print(f\"\\n[SUCCESS] Consolidated and filtered file saved to: {output_file}\")\n    display(filtered_df.head())\nelse:\n    print(\"No CSV files found in the results folder to consolidate.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}