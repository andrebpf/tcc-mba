{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Merge: Sentiment + BOVA11 Returns\n\nThis notebook merges daily sentiment data with logarithmic returns from BOVA11.\n\n## Objectives:\n1. Load sentiment and returns data\n2. Perform date-based merge\n3. Create lagged variables\n4. Preliminary correlation analysis\n5. Save final dataset for statistical analysis\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup & Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\n# Add root directory to path\nsys.path.append(os.path.abspath('..'))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Project modules\nfrom src.sentiment.daily_aggregation import (\n    merge_with_market_data,\n    create_lagged_features,\n    calculate_sentiment_correlation\n)\n\n# Visualization settings\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n%matplotlib inline\n\nprint(\"Imports complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Loading"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Daily Sentiment Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Path to the file generated in the previous notebook\nsentiment_path = '../src/dataset/sentiment/daily_sentiment.csv'\n\nprint(f\"Loading daily sentiment: {sentiment_path}\")\ndf_sentiment = pd.read_csv(sentiment_path)\ndf_sentiment['date'] = pd.to_datetime(df_sentiment['date'])\n\nprint(f\"   Total days: {len(df_sentiment)}\")\nprint(f\"   Period: {df_sentiment['date'].min().date()} to {df_sentiment['date'].max().date()}\")\nprint(f\"\\nFirst rows:\")\ndf_sentiment.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 BOVA11 Returns Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import glob\n\n# Find the most recent log returns file\nmarket_dir = '../src/dataset/market_data/'\nlog_return_files = glob.glob(os.path.join(market_dir, 'BOVA11_log_returns_*.csv'))\n\nif not log_return_files:\n    print(\"ERROR: No log returns files found!\")\n    print(\"   Run first: src/cotation/calculate_log_returns.py\")\nelse:\n    # Get the most recent\n    market_path = max(log_return_files)\n    \n    print(f\"Loading BOVA11 returns: {market_path}\")\n    df_market = pd.read_csv(market_path)\n    df_market['Date'] = pd.to_datetime(df_market['Date'])\n    \n    # Rename to standardize\n    df_market.rename(columns={'Date': 'date'}, inplace=True)\n    \n    print(f\"   Total trading days: {len(df_market)}\")\n    print(f\"   Period: {df_market['date'].min().date()} to {df_market['date'].max().date()}\")\n    print(f\"\\nFirst rows:\")\n    display(df_market.head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Merge"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "END_DATE = datetime(2025, 12, 31)\n# Merge by date (inner join - only days with both data)\nprint(\"Performing merge...\\n\")\n\ndf_merged = merge_with_market_data(\n    sentiment_df=df_sentiment,\n    market_df=df_market,\n    date_column='date',\n    how='inner'  # Only days that have both (news AND trading session)\n)\n\nprint(f\"Merge complete!\")\nprint(f\"   Days in final dataset: {len(df_merged)}\")\nprint(f\"   Period: {df_merged['date'].min().date()} to {df_merged['date'].max().date()}\")\n\n# Check for missing values\nmissing = df_merged.isnull().sum()\nif missing.sum() > 0:\n    print(f\"\\nMissing values:\")\n    print(missing[missing > 0])\nelse:\n    print(f\"\\nNo missing values!\")\n\ndf_merged = df_merged[df_merged['date'] <= END_DATE]\ndf_merged.head(10)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Creating Lagged Variables\n\nWe create lags to test different hypotheses:\n- **Lag 0**: Sentiment at t vs Return at t (contemporary correlation)\n- **Lag 1**: Sentiment at t vs Return at t+1 (sentiment precedes return)\n- **Lag 2, 3**: Longer-term effects"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select sentiment columns to create lags\nsentiment_columns = [\n    'sentiment_mean',\n    'sentiment_std',\n    'sentiment_momentum',\n    'news_count',\n    'count_positive',\n    'count_negative'\n]\n\n# Create lags of 1 to 3 days\ndf_with_lags = create_lagged_features(\n    df=df_merged,\n    columns_to_lag=sentiment_columns,\n    lags=[1, 2, 3],\n    date_column='date'\n)\n\nprint(f\"Lagged variables created!\")\nprint(f\"   Total columns: {len(df_with_lags.columns)}\")\nprint(f\"\\nNew columns created:\")\nlag_cols = [col for col in df_with_lags.columns if 'lag' in col]\nprint(lag_cols[:10])  # Show only the first 10\n\n# Remove rows with NaN (first days without complete lags)\ndf_final = df_with_lags.dropna()\nprint(f\"\\nAfter removing NaN from lags: {len(df_final)} days\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Preliminary Correlation Analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1 Contemporary Correlation (t vs t)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correlation between sentiment and return on the same day\ncorr_t0 = df_final[['sentiment_mean', 'Log_Return']].corr()\n\nprint(\"Contemporary Correlation (t vs t):\")\nprint(f\"\\nsentiment_mean vs Log_Return: {corr_t0.iloc[0, 1]:.4f}\")\n\n# Visualization\nplt.figure(figsize=(8, 6))\nplt.scatter(df_final['sentiment_mean'], df_final['Log_Return'], alpha=0.5)\nplt.xlabel('Average Sentiment Score')\nplt.ylabel('Log Return (BOVA11)')\nplt.title(f'Contemporary Correlation: {corr_t0.iloc[0, 1]:.4f}')\nplt.axhline(y=0, color='red', linestyle='--', linewidth=1)\nplt.axvline(x=0, color='red', linestyle='--', linewidth=1)\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Correlation with Different Lags"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate correlations for different lags\ncorrelation_results = calculate_sentiment_correlation(\n    df=df_final,\n    sentiment_col='sentiment_mean',\n    return_col='Log_Return',\n    max_lag=5\n)\n\nprint(\"Correlation between Sentiment(t) and Return(t+lag):\")\nprint(correlation_results)\n\n# Visualization\nplt.figure(figsize=(10, 6))\nplt.bar(correlation_results['lag'], correlation_results['correlation'], alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=1)\nplt.xlabel('Lag (days)')\nplt.ylabel('Pearson Correlation')\nplt.title('Correlation: Sentiment(t) vs Return(t+lag)')\nplt.grid(True, alpha=0.3, axis='y')\nplt.xticks(correlation_results['lag'])\n\n# Add values on top of bars\nfor idx, row in correlation_results.iterrows():\n    plt.text(row['lag'], row['correlation'], f\"{row['correlation']:.3f}\",\n             ha='center', va='bottom' if row['correlation'] > 0 else 'top')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Correlation Matrix (All Variables)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select relevant columns for correlation\ncorrelation_cols = [\n    'Log_Return',\n    'sentiment_mean',\n    'sentiment_momentum',\n    'news_count',\n    'count_positive',\n    'count_negative',\n    'sentiment_mean_lag1',\n    'sentiment_mean_lag2'\n]\n\n# Calculate correlation matrix\ncorr_matrix = df_final[correlation_cols].corr()\n\n# Visualization\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    corr_matrix,\n    annot=True,\n    fmt='.3f',\n    cmap='coolwarm',\n    center=0,\n    square=True,\n    linewidths=1,\n    cbar_kws={\"shrink\": 0.8}\n)\nplt.title('Correlation Matrix: Sentiment vs Returns', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Show correlations with Log_Return in descending order\nprint(\"\\nCorrelations with Log_Return (sorted):\")\nlog_return_corr = corr_matrix['Log_Return'].drop('Log_Return').sort_values(ascending=False)\nprint(log_return_corr)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Combined Time Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n\n# Chart 1: BOVA11 Returns\naxes[0].plot(df_final['date'], df_final['Log_Return'], linewidth=1, alpha=0.7, label='Log Return')\naxes[0].fill_between(\n    df_final['date'],\n    df_final['Log_Return'],\n    0,\n    where=(df_final['Log_Return'] > 0),\n    alpha=0.3,\n    color='green'\n)\naxes[0].fill_between(\n    df_final['date'],\n    df_final['Log_Return'],\n    0,\n    where=(df_final['Log_Return'] <= 0),\n    alpha=0.3,\n    color='red'\n)\naxes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\naxes[0].set_title('Logarithmic Returns - BOVA11', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('Log Return')\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\n\n# Chart 2: Average Sentiment\naxes[1].plot(df_final['date'], df_final['sentiment_mean'], linewidth=1.5, \n             alpha=0.7, color='purple', label='Sentiment Mean')\naxes[1].fill_between(\n    df_final['date'],\n    df_final['sentiment_mean'],\n    0,\n    where=(df_final['sentiment_mean'] > 0),\n    alpha=0.3,\n    color='green'\n)\naxes[1].fill_between(\n    df_final['date'],\n    df_final['sentiment_mean'],\n    0,\n    where=(df_final['sentiment_mean'] <= 0),\n    alpha=0.3,\n    color='red'\n)\naxes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\naxes[1].set_title('Daily Average Sentiment', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Sentiment Score')\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Observe visually if there are periods where sentiment and return move together!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Save Final Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final output path\nfinal_output_path = '../src/dataset/result/sentiment_returns_merged.csv'\n\n# Save\ndf_final.to_csv(final_output_path, index=False)\n\nprint(f\"Final dataset saved to: {final_output_path}\")\nprint(f\"   Total observations: {len(df_final)}\")\nprint(f\"   Total variables: {len(df_final.columns)}\")\nprint(f\"   Period: {df_final['date'].min().date()} to {df_final['date'].max().date()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Linear Regression Analysis (OLS)\n\nWe use OLS regression (`statsmodels`) to quantify the statistical relationship between news sentiment and BOVA11 returns.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import statsmodels.api as sm\nfrom scipy import stats\n\n# Prepare data for regression - remove NaN (created by lags)\ndf_reg = df_final.dropna().copy()\nprint(f\"Observations available for regression: {len(df_reg)}\")\nprint(f\"Period: {df_reg['date'].min().date()} to {df_reg['date'].max().date()}\")\nprint(f\"\\nAvailable independent variables:\")\nprint(f\"  - sentiment_mean (contemporary sentiment)\")\nprint(f\"  - sentiment_mean_lag1\")\nprint(f\"  - sentiment_mean_lag2\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.1 Simple Linear Regression: Sentiment(t) → Return(t)\n\nWe test the hypothesis that news sentiment on day `t` explains BOVA11 returns on the same day."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Simple Regression: sentiment_mean -> Log_Return ---\nX_simple = sm.add_constant(df_reg['sentiment_mean'])  # add intercept\ny = df_reg['Log_Return']\n\nmodel_simple = sm.OLS(y, X_simple).fit()\nprint(model_simple.summary())\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.2 Visualization: Simple Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Scatter plot with regression line\nax1 = axes[0]\nax1.scatter(df_reg['sentiment_mean'], df_reg['Log_Return'], alpha=0.5, s=20, label='Observations')\nx_line = np.linspace(df_reg['sentiment_mean'].min(), df_reg['sentiment_mean'].max(), 100)\ny_line = model_simple.params['const'] + model_simple.params['sentiment_mean'] * x_line\nax1.plot(x_line, y_line, color='red', linewidth=2, label='OLS Regression')\nax1.set_xlabel('Daily Average Sentiment')\nax1.set_ylabel('Logarithmic Return (BOVA11)')\nax1.set_title('Regression: Sentiment(t) vs Return(t)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Residuals plot\nax2 = axes[1]\nax2.scatter(model_simple.fittedvalues, model_simple.resid, alpha=0.5, s=20)\nax2.axhline(y=0, color='red', linestyle='--', linewidth=1)\nax2.set_xlabel('Fitted Values')\nax2.set_ylabel('Residuals')\nax2.set_title('Residuals Plot')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.3 Multiple Regression: Sentiment(t) + Lags → Return(t)\n\nWe include lagged variables (sentiment_mean_lag1, sentiment_mean_lag2) to check if sentiment from previous days has predictive power over the current return."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Multiple Regression ---\nfeatures = ['sentiment_mean', 'sentiment_mean_lag1', 'sentiment_mean_lag2']\nX_multi = sm.add_constant(df_reg[features])\ny = df_reg['Log_Return']\n\nmodel_multi = sm.OLS(y, X_multi).fit()\nprint(model_multi.summary())\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.4 Pearson Correlation Test (with p-value)\n\nFormal statistical test to verify the significance of the correlation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('=' * 60)\nprint('PEARSON CORRELATION TEST')\nprint('=' * 60)\n\ncorr, pval = stats.pearsonr(df_reg['sentiment_mean'], df_reg['Log_Return'])\nsig = '***' if pval < 0.01 else ('**' if pval < 0.05 else ('*' if pval < 0.1 else ''))\nprint(f\"sentiment_mean                 -> r = {corr:+.4f}, p = {pval:.4f} {sig}\")\n\ncorr, pval = stats.pearsonr(df_reg['sentiment_mean_lag1'], df_reg['Log_Return'])\nsig = '***' if pval < 0.01 else ('**' if pval < 0.05 else ('*' if pval < 0.1 else ''))\nprint(f\"sentiment_mean_lag1            -> r = {corr:+.4f}, p = {pval:.4f} {sig}\")\n\ncorr, pval = stats.pearsonr(df_reg['sentiment_mean_lag2'], df_reg['Log_Return'])\nsig = '***' if pval < 0.01 else ('**' if pval < 0.05 else ('*' if pval < 0.1 else ''))\nprint(f\"sentiment_mean_lag2            -> r = {corr:+.4f}, p = {pval:.4f} {sig}\")\n\nprint('\\n--- Legend: *** p<0.01, ** p<0.05, * p<0.10 ---')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.5 Regression Results Summary\n\nConsolidated statistical metrics for the thesis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Comparative Summary ---\nprint('=' * 70)\nprint('REGRESSION MODELS SUMMARY')\nprint('=' * 70)\n\nprint(f\"\\n{'Model':<40} {'R²':>8} {'Adj R²':>8} {'F-stat':>10} {'p(F)':>12}\")\nprint('-' * 78)\n\nprint(f\"{'Simple (sentiment_mean)':<40} {model_simple.rsquared:>8.4f} {model_simple.rsquared_adj:>8.4f} {model_simple.fvalue:>10.4f} {model_simple.f_pvalue:>12.4e}\")\nprint(f\"{'Multiple (sentiment + lags)':<40} {model_multi.rsquared:>8.4f} {model_multi.rsquared_adj:>8.4f} {model_multi.fvalue:>10.4f} {model_multi.f_pvalue:>12.4e}\")\n\nprint('\\n' + '=' * 70)\nprint('SIMPLE MODEL COEFFICIENTS')\nprint('=' * 70)\nfor param_name, coef in model_simple.params.items():\n    pval = model_simple.pvalues[param_name]\n    sig = '***' if pval < 0.01 else ('**' if pval < 0.05 else ('*' if pval < 0.1 else ''))\n    print(f\"  {param_name:<30} coef = {coef:+.6f}  p = {pval:.4f} {sig}\")\n\nprint('\\n' + '=' * 70)\nprint('MULTIPLE MODEL COEFFICIENTS')\nprint('=' * 70)\nfor param_name, coef in model_multi.params.items():\n    pval = model_multi.pvalues[param_name]\n    sig = '***' if pval < 0.01 else ('**' if pval < 0.05 else ('*' if pval < 0.1 else ''))\n    print(f\"  {param_name:<30} coef = {coef:+.6f}  p = {pval:.4f} {sig}\")\n\nprint('\\n--- Legend: *** p<0.01, ** p<0.05, * p<0.10 ---')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.6 Stationarity Test (Augmented Dickey-Fuller)\n\nBefore trusting correlation and regression results with time series, we must verify whether the series are **stationary**.\nIf they are not, the correlation found may be **spurious** (false).\n\n- **H0**: The series has a unit root (not stationary)\n- **H1**: The series is stationary\n- **Criterion**: If `p-value < 0.05`, we reject H0 → series is stationary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from statsmodels.tsa.stattools import adfuller\n\ndef run_adf_test(series, name):\n    \"\"\"Runs the ADF test and prints formatted results.\"\"\"\n    result = adfuller(series.dropna(), autolag='AIC')\n    adf_stat = result[0]\n    p_value = result[1]\n    used_lag = result[2]\n    n_obs = result[3]\n    critical_values = result[4]\n    \n    status = 'STATIONARY' if p_value < 0.05 else 'NOT STATIONARY'\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ADF Test: {name}\")\n    print(f\"{'='*60}\")\n    print(f\"  ADF Statistic   : {adf_stat:.6f}\")\n    print(f\"  p-value         : {p_value:.6f}\")\n    print(f\"  Lags used       : {used_lag}\")\n    print(f\"  Observations    : {n_obs}\")\n    print(f\"  Critical Values:\")\n    for key, val in critical_values.items():\n        print(f\"    {key}: {val:.6f}\")\n    print(f\"\\n  -> Result: {status}\")\n    \n    return p_value < 0.05\n\n# Test the main series\nprint('STATIONARITY TESTS (ADF)')\nprint('Criterion: p-value < 0.05 -> stationary series')\n\nis_return_stationary = run_adf_test(df_reg['Log_Return'], 'Log_Return (BOVA11 Return)')\nis_sentiment_stationary = run_adf_test(df_reg['sentiment_mean'], 'sentiment_mean (Daily Sentiment)')\n\n# If sentiment is not stationary, create and test the difference\nif not is_sentiment_stationary:\n    print('\\n' + '!'*60)\n    print('WARNING: sentiment_mean is NOT stationary!')\n    print('Creating differenced series: d_sentiment = sentiment(t) - sentiment(t-1)')\n    print('!'*60)\n    \n    df_reg['sentiment_diff'] = df_reg['sentiment_mean'].diff()\n    is_diff_stationary = run_adf_test(df_reg['sentiment_diff'].dropna(), 'd_sentiment_mean (Differenced Sentiment)')\n    \n    if is_diff_stationary:\n        print('\\nThe differenced series IS stationary. Correlation/regression results are reliable.')\n    else:\n        print('\\nEven the differenced series is not stationary. Interpret results with caution.')\nelse:\n    print('\\nBoth series are stationary. Correlation/regression results are reliable.')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.7 Granger Causality Test\n\nThe Granger test evaluates whether one time series **helps predict** another.\nWe test in **both directions**:\n\n1. **Sentiment → Return**: Does news sentiment anticipate market returns?\n2. **Return → Sentiment**: Do market returns influence news sentiment?\n\n- **H0**: Series X does **not** Granger-cause Y\n- **Criterion**: If `p-value < 0.05`, we reject H0 → X Granger-causes Y\n\n> **Important**: If we prove Sentiment → Return (p < 0.05), this supports the central hypothesis of the thesis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from statsmodels.tsa.stattools import grangercausalitytests\n\nmax_lags = 5  # Test from 1 to 5 lags\n\n# Prepare data without NaN\ndf_granger = df_reg[['Log_Return', 'sentiment_mean']].dropna()\n\n# --- Test 1: Sentiment -> Return ---\nprint('=' * 70)\nprint('GRANGER TEST: Sentiment -> Return')\nprint('H0: sentiment_mean does NOT Granger-cause Log_Return')\nprint('=' * 70)\n# grangercausalitytests expects [Y, X] where we test if X causes Y\ngc_sent_to_ret = grangercausalitytests(df_granger[['Log_Return', 'sentiment_mean']], maxlag=max_lags, verbose=True)\n\nprint('\\n')\n\n# --- Test 2: Return -> Sentiment ---\nprint('=' * 70)\nprint('GRANGER TEST: Return -> Sentiment')\nprint('H0: Log_Return does NOT Granger-cause sentiment_mean')\nprint('=' * 70)\ngc_ret_to_sent = grangercausalitytests(df_granger[['sentiment_mean', 'Log_Return']], maxlag=max_lags, verbose=True)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Granger Test Summary ---\nprint('\\n' + '=' * 70)\nprint('SUMMARY: GRANGER CAUSALITY')\nprint('=' * 70)\nprint(f\"{'Direction':<35} {'Lag':>4} {'F-stat':>10} {'p-value':>10} {'Sig':>5}\")\nprint('-' * 70)\n\nfor lag in range(1, max_lags + 1):\n    # Sentiment -> Return\n    f_stat_sr = gc_sent_to_ret[lag][0]['ssr_ftest'][0]\n    p_val_sr = gc_sent_to_ret[lag][0]['ssr_ftest'][1]\n    sig_sr = '***' if p_val_sr < 0.01 else ('**' if p_val_sr < 0.05 else ('*' if p_val_sr < 0.1 else ''))\n    print(f\"{'Sentiment -> Return':<35} {lag:>4} {f_stat_sr:>10.4f} {p_val_sr:>10.4f} {sig_sr:>5}\")\n\nprint('-' * 70)\n\nfor lag in range(1, max_lags + 1):\n    # Return -> Sentiment\n    f_stat_rs = gc_ret_to_sent[lag][0]['ssr_ftest'][0]\n    p_val_rs = gc_ret_to_sent[lag][0]['ssr_ftest'][1]\n    sig_rs = '***' if p_val_rs < 0.01 else ('**' if p_val_rs < 0.05 else ('*' if p_val_rs < 0.1 else ''))\n    print(f\"{'Return -> Sentiment':<35} {lag:>4} {f_stat_rs:>10.4f} {p_val_rs:>10.4f} {sig_rs:>5}\")\n\nprint('\\n--- Legend: *** p<0.01, ** p<0.05, * p<0.10 ---')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.8 Rolling Correlation\n\nThe annual average correlation may hide important variations over time.\nDuring crisis periods, the correlation tends to be stronger.\n\nWe calculate the correlation in **30-day rolling windows** to visualize how the relationship between sentiment and return varies throughout the year."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Rolling Correlation (Window = 30 days) ---\nwindow = 30\n\ndf_rolling = df_reg[['date', 'sentiment_mean', 'Log_Return']].copy()\ndf_rolling = df_rolling.set_index('date').sort_index()\n\n# Calculate rolling correlation\nrolling_corr = df_rolling['sentiment_mean'].rolling(window=window, min_periods=10).corr(df_rolling['Log_Return'])\n\n# Visualization\nfig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n\n# 1. Sentiment over time\nax1 = axes[0]\nax1.plot(df_rolling.index, df_rolling['sentiment_mean'], color='steelblue', linewidth=1, alpha=0.7)\nax1.fill_between(df_rolling.index, df_rolling['sentiment_mean'], alpha=0.3, color='steelblue')\nax1.set_ylabel('Average Sentiment')\nax1.set_title('Daily Sentiment')\nax1.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)\nax1.grid(True, alpha=0.3)\n\n# 2. Return over time\nax2 = axes[1]\ncolors = ['green' if v >= 0 else 'red' for v in df_rolling['Log_Return']]\nax2.bar(df_rolling.index, df_rolling['Log_Return'], color=colors, alpha=0.6, width=1)\nax2.set_ylabel('Log Return')\nax2.set_title('BOVA11 Daily Return')\nax2.grid(True, alpha=0.3)\n\n# 3. Rolling Correlation\nax3 = axes[2]\nax3.plot(rolling_corr.index, rolling_corr.values, color='purple', linewidth=2)\nax3.fill_between(rolling_corr.index, rolling_corr.values, alpha=0.2, color='purple')\nax3.axhline(y=0, color='gray', linestyle='--', linewidth=1)\nax3.axhline(y=rolling_corr.mean(), color='red', linestyle=':', linewidth=1.5, label=f'Mean: {rolling_corr.mean():.3f}')\nax3.set_ylabel(f'Correlation ({window}d)')\nax3.set_xlabel('Date')\nax3.set_title(f'Rolling Correlation (Window = {window} days)')\nax3.legend(loc='upper right')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Rolling correlation statistics\nprint(f'\\nRolling Correlation Statistics ({window} days):')\nprint(f'  Mean     : {rolling_corr.mean():+.4f}')\nprint(f'  Median   : {rolling_corr.median():+.4f}')\nprint(f'  Min      : {rolling_corr.min():+.4f}')\nprint(f'  Max      : {rolling_corr.max():+.4f}')\nprint(f'  Std      : {rolling_corr.std():.4f}')\nprint(f'  %% days>0: {(rolling_corr > 0).sum() / rolling_corr.notna().sum() * 100:.1f}%%')\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}