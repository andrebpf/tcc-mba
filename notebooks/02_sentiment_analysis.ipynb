{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sentiment Analysis with FinBERT-PT-BR\n\nThis notebook implements sentiment analysis of InfoMoney news using the **FinBERT-PT-BR** model (state-of-the-art for finance in Portuguese).\n\n## Objectives:\n1. Load and test the FinBERT-PT-BR model\n2. Process all 11,504 news articles\n3. Perform exploratory analysis of sentiments\n4. Aggregate sentiments by date\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup & Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\n# Fix for Unicode error on Windows\nos.environ[\"PYTHONUTF8\"] = \"1\"\n\n# Add root directory to path to import modules\nsys.path.append(os.path.abspath('..'))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Project modules\nfrom src.sentiment.sentiment_analyzer import (\n    setup_sentiment_model,\n    predict_sentiment,\n    predict_batch,\n    analyze_news_file\n)\nfrom src.sentiment.daily_aggregation import aggregate_daily_sentiment\n\n# Visualization settings\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n%matplotlib inline\n\nprint(\"Imports complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. GPU Verification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"GPU not detected. Processing will be on CPU (slower).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Manual Validation (TCC Requirement)\nComparison of model predictions with manual labels to validate accuracy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- MANUAL MODEL VALIDATION ---\n\n# Load manual validation dataset\nmanual_labels_path = r'../src/dataset/sentiment/news_with_sentiment_manual_labeling.csv'\ndf_manual = pd.read_csv(manual_labels_path)\n\nprint(f\"Loaded validation dataset with {len(df_manual)} samples.\")\n\n# Convert manual sentiment to model format if necessary (-1, 0, 1) -> (negative, neutral, positive)\n# Assuming file has 'sentiment' column with numeric or mappable string values\nlabel_map = {\n    -1: 'negative',\n    0: 'neutral',\n    1: 'positive',\n    '-1': 'negative',\n    '0': 'neutral',\n    '1': 'positive'\n}\n\nif 'sentiment' in df_manual.columns:\n    # Ensure we have comparable labels\n    df_manual['manual_label'] = df_manual['sentiment'].map(label_map)\n    \n    # Load model\n    model, tokenizer, device = setup_sentiment_model()\n    \n    # Make predictions on validation dataset\n    print(\"Making predictions on validation dataset...\")\n    # predict_batch returns a DataFrame with columns: prob_neg, prob_neu, prob_pos, sentiment_score\n    df_results = predict_batch(df_manual['title'].tolist(), model, tokenizer, device)\n    \n    # Extract predicted label from probabilities\n    # Map columns to labels: prob_neg -> negative, prob_neu -> neutral, prob_pos -> positive\n    prob_cols = ['prob_neg', 'prob_neu', 'prob_pos']\n    col_to_label = {'prob_neg': 'negative', 'prob_neu': 'neutral', 'prob_pos': 'positive'}\n    \n    # Find the column with max probability for each row and map to label\n    df_manual['predicted_label'] = df_results[prob_cols].idxmax(axis=1).map(col_to_label)\n    df_manual['predicted_score'] = df_results['sentiment_score']\n    \n    # Calculate metrics\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n    \n    # Remove NaN if any (in case some manual labels were not mapped)\n    df_valid = df_manual.dropna(subset=['manual_label', 'predicted_label'])\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"MANUAL VALIDATION REPORT\")\n    print(\"=\"*50)\n    print(f\"Samples used: {len(df_valid)}\")\n    \n    print(\"\\n--- General Metrics ---\")\n    acc = accuracy_score(df_valid['manual_label'], df_valid['predicted_label'])\n    print(f\"Accuracy: {acc:.4f}\")\n    \n    print(\"\\n--- Classification Report ---\")\n    print(classification_report(df_valid['manual_label'], df_valid['predicted_label']))\n    \n    print(\"\\n--- Confusion Matrix ---\")\n    labels = ['negative', 'neutral', 'positive']\n    cm = confusion_matrix(df_valid['manual_label'], df_valid['predicted_label'], labels=labels)\n    \n    # Plot matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual (Manual)')\n    plt.title('Confusion Matrix: FinBERT vs Manual Classification')\n    plt.show()\n    \n    # Save validation results\n    output_val_path = manual_labels_path.replace('.csv', '_results.csv')\n    df_manual.to_csv(output_val_path, index=False)\n    print(f\"\\nDetailed results saved to: {output_val_path}\")\nelse:\n    print(\"ERROR: 'sentiment' column not found in manual validation file.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Full Dataset Processing\n\nLoad the model:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model\nprint(\"Loading FinBERT-PT-BR model...\\n\")\ntokenizer, model, device = setup_sentiment_model()\n\nprint(f\"\\nModel ready for use!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now let's process the InfoMoney news."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arquivo de entrada: ../src/dataset/scraper/consolidated_news_20260209.csv\n",
      " Arquivo de saída: ../src/dataset/sentiment/news_with_sentiment.csv\n",
      "\n",
      " Processamento iniciado... (isso pode levar alguns minutos)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "END_DATE = datetime(2025, 12, 31)\n",
    "# Caminhos dos arquivos\n",
    "input_path = '../src/dataset/scraper/consolidated_news_20260209.csv'\n",
    "output_path = '../src/dataset/sentiment/news_with_sentiment.csv'\n",
    "\n",
    "print(f\" Arquivo de entrada: {input_path}\")\n",
    "print(f\" Arquivo de saída: {output_path}\")\n",
    "print(\"\\n Processamento iniciado... (isso pode levar alguns minutos)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_input = pd.read_csv(input_path)\ndf_input['date'] = pd.to_datetime(df_input['date'])\ndf_input = df_input[df_input['date'] <= END_DATE]\ninput_path_filtered = input_path.replace('.csv', '_filtered.csv')\ndf_input.to_csv(input_path_filtered, index=False)\n\n# Process all news\n# NOTE: With GPU, should take ~5-10 minutes. With CPU, ~20-30 minutes.\n\ndf_with_sentiment = analyze_news_file(\n    input_csv_path=input_path_filtered,\n    output_csv_path=output_path,\n    text_column='title',\n    batch_size=32  # Adjust to 64 or 128 if you have a powerful GPU\n)\n\nprint(\"\\nProcessing complete!\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Exploratory Data Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load results (in case notebook is restarted)\ndf_with_sentiment = pd.read_csv(output_path)\ndf_with_sentiment['date'] = pd.to_datetime(df_with_sentiment['date'])\n\nprint(f\"Total news processed: {len(df_with_sentiment):,}\")\nprint(f\"Period: {df_with_sentiment['date'].min()} to {df_with_sentiment['date'].max()}\")\nprint(f\"\\nFirst rows:\")\ndf_with_sentiment.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1 Descriptive Statistics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"SENTIMENT STATISTICS\")\nprint(\"=\"*60)\n\nprint(\"\\nSentiment Score:\")\nprint(df_with_sentiment['sentiment_score'].describe())\n\nprint(\"\\nProbabilities:\")\nprint(df_with_sentiment[['prob_neg', 'prob_neu', 'prob_pos']].describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Sentiment Distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Classify news\ndf_with_sentiment['sentiment_class'] = pd.cut(\n    df_with_sentiment['sentiment_score'],\n    bins=[-np.inf, -0.2, 0.2, np.inf],\n    labels=['Negative', 'Neutral', 'Positive']\n)\n\n# Count\nsentiment_counts = df_with_sentiment['sentiment_class'].value_counts()\nprint(\"\\nSentiment Distribution:\")\nprint(sentiment_counts)\nprint(f\"\\nPercentages:\")\nprint(sentiment_counts / len(df_with_sentiment) * 100)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Score histogram\naxes[0, 0].hist(df_with_sentiment['sentiment_score'], bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Neutral')\naxes[0, 0].set_title('Sentiment Score Distribution', fontsize=14, fontweight='bold')\naxes[0, 0].set_xlabel('Sentiment Score')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].legend()\n\n# 2. Classification pie chart\ncolors_pie = ['#e74c3c', '#95a5a6', '#2ecc71']\nsentiment_counts.plot(kind='pie', ax=axes[0, 1], autopct='%1.1f%%', colors=colors_pie, startangle=90)\naxes[0, 1].set_title('Sentiment Classification', fontsize=14, fontweight='bold')\naxes[0, 1].set_ylabel('')\n\n# 3. Probability boxplot\ndf_with_sentiment[['prob_neg', 'prob_neu', 'prob_pos']].boxplot(ax=axes[1, 0])\naxes[1, 0].set_title('Probability Distribution', fontsize=14, fontweight='bold')\naxes[1, 0].set_ylabel('Probability')\naxes[1, 0].set_xticklabels(['Negative', 'Neutral', 'Positive'])\n\n# 4. Monthly time evolution (sample)\ndf_monthly = df_with_sentiment.set_index('date').resample('ME')['sentiment_score'].mean()\ndf_monthly.plot(ax=axes[1, 1], marker='o', linewidth=2)\naxes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\naxes[1, 1].set_title('Monthly Average Sentiment', fontsize=14, fontweight='bold')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].set_ylabel('Average Sentiment Score')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Top Most Positive and Negative News"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"TOP 10 MOST POSITIVE NEWS\")\nprint(\"=\"*80 + \"\\n\")\n\ntop_positive = df_with_sentiment.nlargest(10, 'sentiment_score')[['date', 'title', 'sentiment_score']]\nfor idx, row in top_positive.iterrows():\n    print(f\"Score: {row['sentiment_score']:+.3f} | {row['date'].date()}\")\n    print(f\"  '{row['title']}'\")\n    print()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOP 10 MOST NEGATIVE NEWS\")\nprint(\"=\"*80 + \"\\n\")\n\ntop_negative = df_with_sentiment.nsmallest(10, 'sentiment_score')[['date', 'title', 'sentiment_score']]\nfor idx, row in top_negative.iterrows():\n    print(f\"Score: {row['sentiment_score']:+.3f} | {row['date'].date()}\")\n    print(f\"  '{row['title']}'\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Daily Aggregation\n\nGroup sentiments by date to create the time series that will be correlated with BOVA11 returns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregate by date\ndaily_sentiment = aggregate_daily_sentiment(df_with_sentiment)\n\nprint(f\"Total days with news: {len(daily_sentiment)}\")\nprint(f\"Period: {daily_sentiment['date'].min().date()} to {daily_sentiment['date'].max().date()}\")\nprint(f\"\\nFirst rows:\")\ndaily_sentiment.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.1 Daily Statistics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"DAILY STATISTICS\")\nprint(\"=\"*60 + \"\\n\")\n\nprint(\"News per day:\")\nprint(daily_sentiment['news_count'].describe())\n\nprint(\"\\nDaily average sentiment:\")\nprint(daily_sentiment['sentiment_mean'].describe())\n\nprint(\"\\nSentiment momentum:\")\nprint(daily_sentiment['sentiment_momentum'].describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n\n# 1. Daily average sentiment\naxes[0].plot(daily_sentiment['date'], daily_sentiment['sentiment_mean'], linewidth=1.5, alpha=0.7)\naxes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)\naxes[0].fill_between(\n    daily_sentiment['date'],\n    daily_sentiment['sentiment_mean'],\n    0,\n    where=(daily_sentiment['sentiment_mean'] > 0),\n    alpha=0.3,\n    color='green',\n    label='Positive'\n)\naxes[0].fill_between(\n    daily_sentiment['date'],\n    daily_sentiment['sentiment_mean'],\n    0,\n    where=(daily_sentiment['sentiment_mean'] <= 0),\n    alpha=0.3,\n    color='red',\n    label='Negative'\n)\naxes[0].set_title('Daily Average Sentiment', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('Sentiment Score')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 2. News volume\naxes[1].bar(daily_sentiment['date'], daily_sentiment['news_count'], alpha=0.7)\naxes[1].set_title('News Volume per Day', fontsize=14, fontweight='bold')\naxes[1].set_ylabel('Number of News')\naxes[1].grid(True, alpha=0.3, axis='y')\n\n# 3. Stacked sentiment distribution\naxes[2].bar(daily_sentiment['date'], daily_sentiment['count_negative'], label='Negative', color='#e74c3c')\naxes[2].bar(daily_sentiment['date'], daily_sentiment['count_neutral'], \n            bottom=daily_sentiment['count_negative'], label='Neutral', color='#95a5a6')\naxes[2].bar(daily_sentiment['date'], daily_sentiment['count_positive'],\n            bottom=daily_sentiment['count_negative'] + daily_sentiment['count_neutral'],\n            label='Positive', color='#2ecc71')\naxes[2].set_title('Daily Sentiment Distribution', fontsize=14, fontweight='bold')\naxes[2].set_xlabel('Date')\naxes[2].set_ylabel('Number of News')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.2 Save Aggregated Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save daily aggregation\ndaily_output_path = '../src/dataset/sentiment/daily_sentiment.csv'\ndaily_sentiment.to_csv(daily_output_path, index=False)\n\nprint(f\"Daily data saved to: {daily_output_path}\")\nprint(f\"   Total days: {len(daily_sentiment)}\")\nprint(f\"   Columns: {list(daily_sentiment.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Final Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"SENTIMENT ANALYSIS SUMMARY\")\nprint(\"=\"*80)\n\nprint(f\"\\nProcessed Data:\")\nprint(f\"   - Total news: {len(df_with_sentiment):,}\")\nprint(f\"   - Period: {df_with_sentiment['date'].min().date()} to {df_with_sentiment['date'].max().date()}\")\nprint(f\"   - Days with news: {len(daily_sentiment)}\")\n\nprint(f\"\\nGeneral Statistics:\")\nprint(f\"   - Overall average sentiment: {df_with_sentiment['sentiment_score'].mean():+.4f}\")\nprint(f\"   - Standard deviation: {df_with_sentiment['sentiment_score'].std():.4f}\")\nprint(f\"   - Positive news: {sentiment_counts['Positive']:,} ({sentiment_counts['Positive']/len(df_with_sentiment)*100:.1f}%)\")\nprint(f\"   - Neutral news: {sentiment_counts['Neutral']:,} ({sentiment_counts['Neutral']/len(df_with_sentiment)*100:.1f}%)\")\nprint(f\"   - Negative news: {sentiment_counts['Negative']:,} ({sentiment_counts['Negative']/len(df_with_sentiment)*100:.1f}%)\")\n\nprint(f\"\\nGenerated Files:\")\nprint(f\"   1. {output_path}\")\nprint(f\"      All news with sentiment score\")\nprint(f\"   2. {daily_output_path}\")\nprint(f\"      Daily sentiment aggregation\")\n\nprint(f\"\\nNext step: Run notebook 03_sentiment_market_merge.ipynb\")\nprint(f\"   to merge with BOVA11 data!\")\nprint(\"\\n\" + \"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}